#!/bin/bash

! [ -d ~/.local/share/fwikis ] && mkdir ~/.local/share/fwikis
url=$(sed 's/\/[^\.]*$//g' <<< $1)
next=$1

cd /tmp
i=0
nl=0
 
# if there are less than 3 links with wm-allpages-nav and it
# is not the first page then there is no next page link
while :; do
	echo "$next"
	wget -q "$next" -O index
	grep -o 'redirect"><a href="/[^"]*"\|<li><a href="/[^"]*"' 'index' | grep -o '[^/]*$' | sed 's/.$//' >> /tmp/fwtitles
	if [ $i -eq 0 ]; then
	next="$url$(grep mw-allpages-nav index | grep -o 'href="[^"]*' | head -n1 | sed 's/href="//')"
	[ -z $next ] && break
	else
	next="$(grep mw-allpages-nav index | grep -o 'href="[^"]*')"
	[ $(wc -l <<< $next) -lt 4 ] && break
	next=$url$(awk 'NR==2' <<< $next | sed 's/href="//')
	fi
	i=$((i+1))
done

sed 's/^$//g' -zi fwtitles

limit=$2
[ -z $limit ] && limit=3000
split fwtitles -l $limit tmpw_

for req in $(ls -1 tmpw_*);
do
	sed -zi "s/\n/%0D%0A/g;s/'/%27/g;s/,/%2C/g;s/(/%28/g;s/)/%29/g;s/\//%2F/g;s/_/+/g;s/&amp;/%26/g;\
	s/^/catname=\&pages=/;s/$/\&curonly=1\&wpEditToken=%2B%5C\&title=Special%3AExport/" $req
	wget $(sed 's/\/$//;s/[^/]*$//' <<< $1)Special:Export --post-file=$req -O "fp_$req"
done

echo -n "filename:"
read title

cd ~/.local/share/fwikis
xmlmerge /tmp/fp_tmpw_* > "$title"
sed -i 's/ns0\://g' "$title"
sed -i '1c\<mediawiki>' "$title"

cd /tmp
rm tmpw_*
rm fp_tmpw*
rm fwtitles
rm index
echo "done with $title"
